# Papers for Crack

1. [A Probabilistic Model for Information Storage and Organization in the Brain – Rosenblatt (1958)](https://citeseerx.ist.psu.edu/document?doi=10.1.1.588.6448)
2. [Learning Representations by Back-propagating Errors – Rumelhart, Hinton & Williams (1986)](https://www.nature.com/articles/323533a0)
3. [Support-Vector Networks – Cortes & Vapnik (1995)](https://link.springer.com/article/10.1007/BF00994018)
4. [Random Forests – Breiman (2001)](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
5. [XGBoost: A Scalable Tree Boosting System – Chen & Guestrin (2016)](https://dl.acm.org/doi/10.1145/2939672.2939785)
6. [CatBoost: Unbiased Boosting with Categorical Features – Prokhorenkova et al. (2018)](https://arxiv.org/abs/1706.09516)
7. [Visualizing Data using t-SNE – van der Maaten & Hinton (2008)](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
8. [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction – McInnes et al. (2018)](https://arxiv.org/abs/1802.03426)
9. [Bayesian Network Classifiers – Friedman, Geiger & Goldszmidt (1997)](https://www.sciencedirect.com/science/article/abs/pii/S0004370296000431)
10. [AdaBoost: A Boosting Technique for Machine Learning – Freund & Schapire (1997)](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf)

# Deep Learning Fundamentals

11. [Gradient-Based Learning Applied to Document Recognition – LeCun et al. (1998)](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)
12. [A Fast Learning Algorithm for Deep Belief Nets – Hinton, Osindero & Teh (2006)](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
13. [Greedy Layer-Wise Training of Deep Networks – Bengio et al. (2007)](https://dl.acm.org/doi/10.1145/1273496.1273556)
14. [Adam: A Method for Stochastic Optimization – Kingma & Ba (2014)](https://arxiv.org/abs/1412.6980)
15. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting – Srivastava et al. (2014)](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
16. [Batch Normalization: Accelerating Deep Network Training – Ioffe & Szegedy (2015)](https://arxiv.org/abs/1502.03167)
17. [Layer Normalization – Ba et al. (2016)](https://arxiv.org/abs/1607.06450)
18. [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks – Frankle & Carbin (2019)](https://arxiv.org/abs/1803.03635)
19. [Rectified Linear Units Improve Restricted Boltzmann Machines – Nair & Hinton (2010)](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf)
20. [Deep Learning – LeCun, Bengio, & Hinton (2015)](https://www.nature.com/articles/nature14539)

# Computer Vision

21. [ImageNet Classification with Deep Convolutional Neural Networks – Krizhevsky, Sutskever & Hinton (2012)](https://dl.acm.org/doi/10.1145/3065386)
22. [Deep Residual Learning for Image Recognition – He et al. (2016)](https://arxiv.org/abs/1512.03385)
23. [YOLO: You Only Look Once - Unified, Real-Time Object Detection – Redmon et al. (2016)](https://arxiv.org/abs/1506.02640)
24. [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks – Tan & Le (2019)](https://arxiv.org/abs/1905.11946)
25. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale – Dosovitskiy et al. (2020)](https://arxiv.org/abs/2010.11929)
26. [Segment Anything – Kirillov et al. (2023)](https://arxiv.org/abs/2304.02643)
27. [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows – Liu et al. (2021)](https://arxiv.org/abs/2103.14030)
28. [UNet: Convolutional Networks for Biomedical Image Segmentation – Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597)
29. [Self-Supervised Learning of Pretext-Invariant Representations – Grill et al. (2020)](https://arxiv.org/abs/2006.07733)
30. [CycleGAN: Unpaired Image-to-Image Translation – Zhu et al. (2017)](https://arxiv.org/abs/1703.10593)

# Natural Language Processing (NLP)

31. [Attention is All You Need – Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762)
32. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding – Devlin et al. (2018)](https://arxiv.org/abs/1810.04805)
33. [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer – Raffel et al. (2020)](https://arxiv.org/abs/1910.10683)
34. [GPT-3: Language Models are Few-Shot Learners – Brown et al. (2020)](https://arxiv.org/abs/2005.14165)
35. [BLOOM: A 176B Parameter Open-Access Multilingual Language Model – BigScience (2022)](https://arxiv.org/abs/2211.05100)
36. [ELMo: Deep Contextualized Word Representations – Peters et al. (2018)](https://arxiv.org/abs/1802.05365)
37. [RoBERTa: A Robustly Optimized BERT Pretraining Approach – Liu et al. (2019)](https://arxiv.org/abs/1907.11692)
38. [Word2Vec: Distributed Representations of Words and Phrases – Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781)
39. [XLNet: Generalized Autoregressive Pretraining for Language Understanding – Yang et al. (2019)](https://arxiv.org/abs/1906.08237)
40. [Flan-T5: Scaling Instruction-Finetuned Language Models – Chung et al. (2022)](https://arxiv.org/abs/2210.11416)

# Generative Models

41. [Generative Adversarial Nets – Goodfellow et al. (2014)](https://arxiv.org/abs/1406.2661)
42. [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks – Karras et al. (2019)](https://arxiv.org/abs/1812.04948)
43. [Auto-Encoding Variational Bayes – Kingma & Welling (2013)](https://arxiv.org/abs/1312.6114)
44. [DALL·E: Creating Images from Text – Ramesh et al. (2021)](https://arxiv.org/abs/2102.12092)
45. [Diffusion Models Beat GANs on Image Synthesis – Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233)
46. [Flow-Based Generative Models – Dinh et al. (2014)](https://arxiv.org/abs/1410.8516)

# Reinforcement Learning

47. [Mastering the Game of Go with Deep Neural Networks and Tree Search – Silver et al. (2016)](https://www.nature.com/articles/nature16961)
48. [Proximal Policy Optimization Algorithms – Schulman et al. (2017)](https://arxiv.org/abs/1707.06347)
49. [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments – Lowe et al. (2017)](https://arxiv.org/abs/1706.02275)
50. [Deep Reinforcement Learning from Human Preferences – Christiano et al. (2017)](https://arxiv.org/abs/1706.03741)
