# Papers for Crack

## Table of Contents

1. [Fundamentals](#fundamentals)
2. [Deep Learning](#deep-learning)
3. [Computer Vision](#computer-vision)
4. [Natural Language Processing](#natural-language-processing)
5. [Generative Models](#generative-models)
6. [Reinforcement Learning](#reinforcement-learning)
7. [Optimization and Training Techniques](#optimization-and-training-techniques)
8. [Other Important Areas](#other-important-areas)
9. [AI Reasoning](#ai-reasoning)
10. [Security](#security)
11. [Additional Resources](#additional-resources)

## Fundamentals

1. [A Probabilistic Model for Information Storage and Organization in the Brain](https://citeseerx.ist.psu.edu/document?doi=10.1.1.588.6448) - Rosenblatt (1958)
2. [Learning Representations by Back-propagating Errors](https://www.nature.com/articles/323533a0) - Rumelhart, Hinton & Williams (1986)
3. [Support-Vector Networks](https://link.springer.com/article/10.1007/BF00994018) - Cortes & Vapnik (1995)
4. [Random Forests](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) - Breiman (2001)
5. [XGBoost: A Scalable Tree Boosting System](https://dl.acm.org/doi/10.1145/2939672.2939785) - Chen & Guestrin (2016)
6. [CatBoost: Unbiased Boosting with Categorical Features](https://arxiv.org/abs/1706.09516) - Prokhorenkova et al. (2018)
7. [Visualizing Data using t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) - van der Maaten & Hinton (2008)
8. [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) - McInnes et al. (2018)
9. [Bayesian Network Classifiers](https://www.sciencedirect.com/science/article/abs/pii/S0004370296000431) - Friedman, Geiger & Goldszmidt (1997)
10. [AdaBoost: A Boosting Technique for Machine Learning](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) - Freund & Schapire (1997)
11. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Christopher Olah
12. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy
13. [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](https://dl.acm.org/doi/10.5555/2987189.2987190) - Hinton & van Camp (1993)
14. [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/abs/math/0406077) - Peter Grünwald (2004)

## Deep Learning

15. [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) - LeCun et al. (1998)
16. [A Fast Learning Algorithm for Deep Belief Nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) - Hinton, Osindero & Teh (2006)
17. [Greedy Layer-Wise Training of Deep Networks](https://dl.acm.org/doi/10.1145/1273496.1273556) - Bengio et al. (2007)
18. [Rectified Linear Units Improve Restricted Boltzmann Machines](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf) - Nair & Hinton (2010)
19. [Deep Learning](https://www.nature.com/articles/nature14539) - LeCun, Bengio & Hinton (2015)
20. [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) - Frankle & Carbin (2019)
21. [ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/10.1145/3065386) - Krizhevsky, Sutskever & Hinton (2012)
22. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - He et al. (2015)
23. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. (2017)
24. [Neural Turing Machines](https://arxiv.org/abs/1410.5401) - Graves et al. (2014)
25. [Pointer Networks](https://arxiv.org/abs/1506.03134) - Vinyals et al. (2015)
26. [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) - Vinyals et al. (2016)
27. [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578) - Zoph & Le (2016)
28. [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965) - Huang et al. (2019)
29. [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) - Yu & Koltun (2016)
30. [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731) - Chen et al. (2017)
31. [A Simple Neural Network Module for Relational Reasoning](https://arxiv.org/abs/1706.01427) - Santoro et al. (2017)
32. [Relational Recurrent Neural Networks](https://arxiv.org/abs/1806.01822) - Santoro et al. (2018)
33. [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) - Amodei et al. (2016)
34. [DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter](https://arxiv.org/abs/1910.01108) - Sanh et al. (2019)
35. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) - Hinton, Vinyals & Dean (2015)

## Computer Vision

36. [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - Simonyan & Zisserman (2014)
37. [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842) - Szegedy et al. (2015)
38. [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) - Huang et al. (2017)
39. [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) - Tan & Le (2019)
40. [YOLO: You Only Look Once](https://arxiv.org/abs/1506.02640) - Redmon et al. (2016)
41. [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) - Ren et al. (2015)
42. [Mask R-CNN](https://arxiv.org/abs/1703.06870) - He et al. (2017)
43. [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) - Long et al. (2015)
44. [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) - Ronneberger et al. (2015)
45. [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets](https://arxiv.org/abs/1606.00915) - Chen et al. (2017)
46. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) - Dosovitskiy et al. (2020)
47. [Segment Anything](https://arxiv.org/abs/2304.02643) - Kirillov et al. (2023)
48. [Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows](https://arxiv.org/abs/2103.14030) - Liu et al. (2021)
49. [Self-Supervised Learning of Pretext-Invariant Representations](https://arxiv.org/abs/2006.07733) - Grill et al. (2020)
50. [CycleGAN: Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) - Zhu et al. (2017)
51. [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) - He et al. (2016)
52. [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934) - Mildenhall et al. (2020)

## Natural Language Processing

53. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) - Devlin et al. (2018)
54. [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) - Raffel et al. (2020)
55. [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - Brown et al. (2020)
56. [BLOOM: A 176B Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) - BigScience (2022)
57. [ELMo: Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365) - Peters et al. (2018)
58. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) - Liu et al. (2019)
59. [Word2Vec: Distributed Representations of Words and Phrases](https://arxiv.org/abs/1301.3781) - Mikolov et al. (2013)
60. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) - Yang et al. (2019)
61. [FLAN-T5: Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) - Chung et al. (2022)
62. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) - Sutskever et al. (2014)
63. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) - Bahdanau et al. (2015)
64. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) - Pennington et al. (2014)
65. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) - Lan et al. (2020)
66. [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf) - Yang et al. (2016)

## Generative Models

67. [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661) - Goodfellow et al. (2014)
68. [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) - Kingma & Welling (2013)
69. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) - Radford et al. (2016)
70. [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) - Mirza & Osindero (2014)
71. [Wasserstein GAN](https://arxiv.org/abs/1701.07875) - Arjovsky et al. (2017)
72. [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) - Karras et al. (2019)
73. [DALL·E: Creating Images from Text](https://arxiv.org/abs/2102.12092) - Ramesh et al. (2021)
74. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233) - Dhariwal & Nichol (2021)
75. [Flow-Based Generative Models](https://arxiv.org/abs/1410.8516) - Dinh et al. (2015)
76. [DDPM: Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) - Ho et al. (2020)
77. [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487) - Saharia et al. (2022)
78. [Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control](https://arxiv.org/pdf/2405.05852) - (2023)
79. [Neural Radiance Fields for Unconstrained Photo Collections](https://arxiv.org/abs/2008.02268) - Martin-Brualla et al. (2020)

## Reinforcement Learning

80. [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) - Mnih et al. (2013)
81. [Human-Level Control Through Deep Reinforcement Learning](https://www.nature.com/articles/nature14236) - Mnih et al. (2015)
82. [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) - Schulman et al. (2017)
83. [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) - Mnih et al. (2016)
84. [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) - Lillicrap et al. (2016)
85. [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.nature.com/articles/nature16961) - Silver et al. (2016)
86. [Mastering the Game of Go Without Human Knowledge](https://www.nature.com/articles/nature24270) - Silver et al. (2017)
87. [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275) - Lowe et al. (2017)
88. [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741) - Christiano et al. (2017)
89. [Deep Reinforcement Learning with Double Q-Learning](https://arxiv.org/abs/1509.06461) - van Hasselt et al. (2016)
90. [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) - Wang et al. (2016)
91. [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815) - Silver et al. (2018)
92. [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290) - Haarnoja et al. (2018)

## Optimization and Training Techniques

93. [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) - Kingma & Ba (2015)
94. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) - Srivastava et al. (2014)
95. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) - Ioffe & Szegedy (2015)
96. [Layer Normalization](https://arxiv.org/abs/1607.06450) - Ba et al. (2016)
97. [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329) - Zaremba et al. (2014)
98. [Lookahead Optimizer: k Steps Forward, 1 Step Back](https://arxiv.org/abs/1907.08610) - Zhang et al. (2019)
99. [Adadelta: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701) - Zeiler (2012)
100. [RAdam: On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265) - Liu et al. (2019)

## Other Important Areas

101. [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) - Gatys et al. (2015)
102. [DeepFace: Closing the Gap to Human-Level Performance in Face Verification](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf) - Taigman et al. (2014)
103. [Siamese Neural Networks for One-Shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) - Koch et al. (2015)
104. [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025) - Jaderberg et al. (2015)
105. [Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization](https://arxiv.org/abs/1610.02391) - Selvaraju et al. (2017)
106. [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722) - He et al. (2020)
107. [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709) - Chen et al. (2020)
108. [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/abs/2006.09882) - Caron et al. (2020)
109. [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) - Kipf & Welling (2017)
110. [Graph Attention Networks](https://arxiv.org/abs/1710.10903) - Veličković et al. (2018)
111. [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400) - Finn et al. (2017)
112. [Prototypical Networks for Few-Shot Learning](https://arxiv.org/abs/1703.05175) - Snell et al. (2017)
113. [Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding](https://arxiv.org/abs/1810.02338) - Yi et al. (2018)
114. [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) - Hochreiter & Schmidhuber (1997)
115. [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) - Gilmer et al. (2017)
116. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Kaplan et al. (2020)
117. [Kolmogorov Complexity and Algorithmic Randomness](https://www.math.nyu.edu/faculty/artemov/103/KolmogorovComplexity.pdf) - Shen et al.
118. [Machine Super Intelligence](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf) - Shane Legg
119. [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366) - Chen et al. (2018)

## AI Reasoning

120. [Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050) - (2023)
121. [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) - Yao et al. (2023)
122. [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625) - Zhou et al. (2022)
123. [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) - Zeng et al. (2023)
124. [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2308.15240) - Huang et al. (2023)
125. [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2303.05378) - Chen et al. (2023)

## Security

126. [Privacy Preserving Machine Learning: Threats and Solutions](https://arxiv.org/pdf/1804.11238) - Chaudhuri & Monteleoni (2018)
127. [SoK: Security and Privacy in Machine Learning](https://ieeexplore.ieee.org/document/8406613) - Papernot et al. (2018)
128. [Differential Privacy in Deep Learning: An Overview](https://ieeexplore.ieee.org/document/9044259) - Abadi et al. (2020)

## Additional Resources

129. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - Rush et al.
130. [The First Law of Complexodynamics](https://www.scottaaronson.com/papers/cl.pdf) - Scott Aaronson
131. [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton](https://www.scottaaronson.com/papers/coffee.pdf) - Aaronson et al.
132. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
