# Papers for Crack


# Essential AI Papers and Resources

This repository contains a curated list of essential papers and resources for artificial intelligence, machine learning, and deep learning. It covers a wide range of topics from foundational concepts to advanced techniques and specialized applications.

## Table of Contents

1. [Fundamentals](#fundamentals)
2. [Deep Learning](#deep-learning)
3. [Computer Vision](#computer-vision)
4. [Natural Language Processing](#natural-language-processing)
5. [Generative Models](#generative-models)
6. [Reinforcement Learning](#reinforcement-learning)
7. [Optimization and Training Techniques](#optimization-and-training-techniques)
8. [Other Important Areas](#other-important-areas)
9. [Additional Resources](#additional-resources)

## Fundamentals

1. [A Probabilistic Model for Information Storage and Organization in the Brain](https://citeseerx.ist.psu.edu/document?doi=10.1.1.588.6448) - Rosenblatt (1958)
2. [Learning Representations by Back-propagating Errors](https://www.nature.com/articles/323533a0) - Rumelhart, Hinton & Williams (1986)
3. [Support-Vector Networks](https://link.springer.com/article/10.1007/BF00994018) - Cortes & Vapnik (1995)
4. [Random Forests](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) - Breiman (2001)
5. [XGBoost: A Scalable Tree Boosting System](https://dl.acm.org/doi/10.1145/2939672.2939785) - Chen & Guestrin (2016)
6. [CatBoost: Unbiased Boosting with Categorical Features](https://arxiv.org/abs/1706.09516) - Prokhorenkova et al. (2018)
7. [Visualizing Data using t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) - van der Maaten & Hinton (2008)
8. [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) - McInnes et al. (2018)
9. [Bayesian Network Classifiers](https://www.sciencedirect.com/science/article/abs/pii/S0004370296000431) - Friedman, Geiger & Goldszmidt (1997)
10. [AdaBoost: A Boosting Technique for Machine Learning](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) - Freund & Schapire (1997)
11. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Christopher Olah
12. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy
13. [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](https://dl.acm.org/doi/10.5555/2987189.2987190) - Geoffrey E. Hinton and Drew van Camp
14. [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/abs/math/0406077) - Peter Grunwald

## Deep Learning

15. [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) - LeCun et al. (1998)
16. [A Fast Learning Algorithm for Deep Belief Nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) - Hinton, Osindero & Teh (2006)
17. [Greedy Layer-Wise Training of Deep Networks](https://dl.acm.org/doi/10.1145/1273496.1273556) - Bengio et al. (2007)
18. [Rectified Linear Units Improve Restricted Boltzmann Machines](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf) - Nair & Hinton (2010)
19. [Deep Learning](https://www.nature.com/articles/nature14539) - LeCun, Bengio, & Hinton (2015)
20. [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) - Frankle & Carbin (2019)
21. [ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/10.1145/3065386) - Krizhevsky, Sutskever & Hinton (2012)
22. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - He et al. (2015)
23. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. (2017)
24. [Neural Turing Machines](https://arxiv.org/abs/1410.5401) - Alex Graves, et al.
25. [Pointer Networks](https://arxiv.org/abs/1506.03134) - Oriol Vinyals, et al.
26. [Order Matters: Sequence to sequence for sets](https://arxiv.org/abs/1511.06391) - Oriol Vinyals, et al.
27. [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578) - Barret Zoph, et al.
28. [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965) - Yanping Huang, et al.
29. [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) - Fisher Yu and Vladlen Koltun
30. [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731) - Xi Chen, et al.
31. [A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427) - Adam Santoro, et al.
32. [Relational recurrent neural networks](https://arxiv.org/abs/1806.01822) - Adam Santoro, et al.
33. [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) - Dario Amodei, et al.

## Computer Vision

34. [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - Simonyan & Zisserman (2014)
35. [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842) - Szegedy et al. (2014)
36. [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) - Huang et al. (2017)
37. [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) - Tan & Le (2019)
38. [YOLO: You Only Look Once - Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) - Redmon et al. (2016)
39. [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) - Ren et al. (2015)
40. [Mask R-CNN](https://arxiv.org/abs/1703.06870) - He et al. (2017)
41. [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) - Long et al. (2015)
42. [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) - Ronneberger et al. (2015)
43. [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915) - Chen et al. (2017)
44. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) - Dosovitskiy et al. (2020)
45. [Segment Anything](https://arxiv.org/abs/2304.02643) - Kirillov et al. (2023)
46. [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) - Liu et al. (2021)
47. [Self-Supervised Learning of Pretext-Invariant Representations](https://arxiv.org/abs/2006.07733) - Grill et al. (2020)
48. [CycleGAN: Unpaired Image-to-Image Translation](https://arxiv.org/abs/1703.10593) - Zhu et al. (2017)
49. [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) - Kaiming He, et al.

## Natural Language Processing

50. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) - Devlin et al. (2018)
51. [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) - Raffel et al. (2020)
52. [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - Brown et al. (2020)
53. [BLOOM: A 176B Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) - BigScience (2022)
54. [ELMo: Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365) - Peters et al. (2018)
55. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) - Liu et al. (2019)
56. [Word2Vec: Distributed Representations of Words and Phrases](https://arxiv.org/abs/1301.3781) - Mikolov et al. (2013)
57. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) - Yang et al. (2019)
58. [Flan-T5: Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) - Chung et al. (2022)
59. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) - Sutskever et al. (2014)
60. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) - Bahdanau et al. (2014)

## Generative Models

61. [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661) - Goodfellow et al. (2014)
62. [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) - Kingma & Welling (2013)
63. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) - Radford et al. (2015)
64. [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) - Mirza & Osindero (2014)
65. [Wasserstein GAN](https://arxiv.org/abs/1701.07875) - Arjovsky et al. (2017)
66. [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) - Karras et al. (2019)
67. [DALLÂ·E: Creating Images from Text](https://arxiv.org/abs/2102.12092) - Ramesh et al. (2021)
68. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233) - Dhariwal & Nichol (2021)
69. [Flow-Based Generative Models](https://arxiv.org/abs/1410.8516) - Dinh et al. (2014)

## Reinforcement Learning

70. [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) - Mnih et al. (2013)
71. [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) - Mnih et al. (2015)
72. [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) - Schulman et al. (2017)
73. [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) - Mnih et al. (2016)
74. [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971) - Lillicrap et al. (2015)
75. [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.nature.com/articles/nature16961) - Silver et al. (2016)
76. [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275) - Lowe et al. (2017)
77. [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741) - Christiano et al. (2017)
78. [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) - Hado van Hasselt, et al.
79. [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) - Ziyu Wang, et al.
80. [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) - David Silver, et al.

## Optimization and Training Techniques

81. [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) - Kingma & Ba (2014)
82. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) - Srivastava et al. (2014)
83. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) - Ioffe & Szegedy (2015)
84. [Layer Normalization](https://arxiv.org/abs/1607.06450) - Ba et al. (2016)
85. [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329) - Wojciech Zaremba, et al.

## Other Important Areas

86. [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) - Gatys et al. (2015)
87. [DeepFace: Closing the Gap to Human-Level Performance in Face Verification](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf) - Taigman et al. (2014)
88. [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) - Koch et al. (2015)
89. [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025) - Jaderberg et al. (2015)
90. [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391) - Selvaraju et al. (2017)
91. [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722) - He et al. (2019)
92. [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709) - Chen et al. (2020)
93. [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/abs/2006.09882) - Caron et al. (2020)
94. [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) - Kipf & Welling (2016)
95. [Graph Attention Networks](https://arxiv.org/abs/1710.10903) - VeliÄkoviÄ et al. (2017)
96. [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400) - Finn et al. (2017)
97. [Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175) - Snell et al. (2017)
98. [Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding](https://arxiv.org/abs/1810.02338) - Yi et al. (2018)
99. [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) - Hochreiter & Schmidhuber (1997)
100. [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) - Justin Gilmer, et al.
101. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Jared Kaplan, et al.
102. [Kolmogorov Complexity and Algorithmic Randomness](https://www.math.nyu.edu/faculty/artemov/103/KolmogorovComplexity.pdf) - A. Shen, V. A. Uspensky, and N. Vereshchagin
103. [Machine Super Intelligence](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf) - Shane Legg

## Additional Resources

104. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - Sasha Rush, et al.
105. [The First Law of Complexodynamics](https://www.scottaaronson.com/papers/cl.pdf) - Scott Aaronson
106. [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton](https://www.scottaaronson.com/papers/coffee.pdf) - Scott Aaronson, et al.
107. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
